# System Design - High Level Design (HLD) Solutions

Complete HLD solutions for 10 common system design interview questions with diagrams, architecture decisions, and trade-offs.

---

## 1. Music Streaming Application

### Requirements
- Fetch top trending songs
- Regional filtering
- Scalability and fault tolerance
- Proper data schema and DB choices

### Architecture Diagram

```
┌─────────────┐
│   Client    │
└──────┬──────┘
       │
┌──────▼──────────────────────────────────────┐
│          Load Balancer (ALB)                │
└──────┬──────────────────────────────────────┘
       │
┌──────▼──────────┐     ┌─────────────────┐
│   API Gateway   │────▶│  Auth Service   │
└──────┬──────────┘     └─────────────────┘
       │
       ├──────────────────────────────────────┐
       │                                      │
┌──────▼────────────┐            ┌───────────▼──────────┐
│ Trending Service  │            │  Streaming Service   │
└──────┬────────────┘            └───────────┬──────────┘
       │                                     │
┌──────▼──────────┐              ┌───────────▼──────────┐
│  Redis Cache    │              │   CDN (CloudFront)   │
│  (Top 100/reg)  │              │   (Audio Files)      │
└──────┬──────────┘              └──────────────────────┘
       │
┌──────▼──────────────────────────┐
│   Database Layer                │
│                                 │
│  ┌──────────────────────────┐  │
│  │  Song Metadata (Postgres)│  │
│  │  - song_id, title, artist│  │
│  │  - region, genre         │  │
│  └──────────────────────────┘  │
│                                 │
│  ┌──────────────────────────┐  │
│  │  Plays/Analytics (Cassand│  │
│  │  - song_id, user_id      │  │
│  │  - play_count, timestamp │  │
│  └──────────────────────────┘  │
└─────────────────────────────────┘
       │
┌──────▼──────────┐
│  Kafka Stream   │
│  (Play Events)  │
└──────┬──────────┘
       │
┌──────▼────────────┐
│  Spark/Flink      │
│  (Trending Calc)  │
└───────────────────┘
```

### Data Schema

**Song Metadata (PostgreSQL)**
```sql
CREATE TABLE songs (
    song_id BIGSERIAL PRIMARY KEY,
    title VARCHAR(255) NOT NULL,
    artist VARCHAR(255) NOT NULL,
    album VARCHAR(255),
    duration_seconds INT,
    genre VARCHAR(100),
    region VARCHAR(10),
    release_date DATE,
    audio_file_url TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_songs_region ON songs(region);
CREATE INDEX idx_songs_genre ON songs(genre);
```

**Play Analytics (Cassandra)**
```cql
CREATE TABLE play_events (
    song_id BIGINT,
    user_id BIGINT,
    played_at TIMESTAMP,
    play_duration_seconds INT,
    region TEXT,
    PRIMARY KEY ((song_id, region), played_at)
) WITH CLUSTERING ORDER BY (played_at DESC);

CREATE TABLE trending_songs (
    region TEXT,
    time_bucket TEXT, -- hourly, daily
    song_id BIGINT,
    play_count COUNTER,
    PRIMARY KEY ((region, time_bucket), play_count, song_id)
) WITH CLUSTERING ORDER BY (play_count DESC);
```

### Key Design Decisions

1. **Regional Filtering**
   - Partition Redis cache by region: `trending:IN`, `trending:US`, `trending:UK`
   - Store region in song metadata for filtering
   - Regional CDN edge locations for low latency

2. **Scalability**
   - Horizontal scaling of all microservices
   - CDN for static audio content (90%+ traffic offload)
   - Database sharding: Cassandra partitioned by (song_id, region)
   - Read replicas for PostgreSQL

3. **Fault Tolerance**
   - Multi-AZ deployment for all services
   - Redis Sentinel for cache high availability
   - Circuit breaker pattern for external dependencies
   - Dead letter queue for failed events

4. **Database Choices**
   - **PostgreSQL**: Song metadata (ACID required, complex queries)
   - **Cassandra**: Play events (high write throughput, time-series)
   - **Redis**: Caching layer (sub-millisecond reads)

5. **Trending Calculation**
   - Batch job runs every 15 minutes
   - Weighted score: `score = play_count * e^(-decay * age_hours)`
   - Top 100 cached per region with TTL

### API Endpoints

```
GET  /api/v1/trending?region=IN&limit=50
GET  /api/v1/songs/:id/stream
POST /api/v1/songs/:id/play
GET  /api/v1/search?q=song_name&region=IN
```

---

## 2. Hotel Searching System

### Requirements
- Add/remove hotels functionality
- Scaling and caching
- Rate limiting
- Trade-offs analysis

### Architecture Diagram

```
┌──────────────┐
│    Client    │
└──────┬───────┘
       │
┌──────▼────────────────────┐
│   API Gateway + WAF       │
│   (Rate Limiting)         │
└──────┬────────────────────┘
       │
┌──────▼─────────────────┐
│  Search Service        │
│  (Elasticsearch)       │
│  - location, price     │
│  - amenities, rating   │
└──────┬─────────────────┘
       │
       ├────────────────────────┐
       │                        │
┌──────▼───────────┐   ┌────────▼──────────┐
│  Redis Cache     │   │  Inventory Mgmt   │
│  (Popular srch)  │   │  (Add/Remove)     │
└──────────────────┘   └────────┬──────────┘
                                │
                       ┌────────▼──────────┐
                       │  Primary DB       │
                       │  (PostgreSQL)     │
                       │  - hotels         │
                       │  - rooms          │
                       │  - availability   │
                       └───────────────────┘
```

### Data Schema

```sql
CREATE TABLE hotels (
    hotel_id BIGSERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    address TEXT,
    city VARCHAR(100),
    country VARCHAR(100),
    latitude DECIMAL(10, 8),
    longitude DECIMAL(11, 8),
    star_rating DECIMAL(2, 1),
    average_price DECIMAL(10, 2),
    amenities JSONB,
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_hotels_location ON hotels(city, country);
CREATE INDEX idx_hotels_price ON hotels(average_price);
CREATE INDEX idx_hotels_active ON hotels(is_active);

CREATE TABLE rooms (
    room_id BIGSERIAL PRIMARY KEY,
    hotel_id BIGINT REFERENCES hotels(hotel_id),
    room_type VARCHAR(100),
    max_occupancy INT,
    price_per_night DECIMAL(10, 2),
    total_rooms INT,
    is_active BOOLEAN DEFAULT true
);

CREATE TABLE availability (
    availability_id BIGSERIAL PRIMARY KEY,
    room_id BIGINT REFERENCES rooms(room_id),
    date DATE,
    available_rooms INT,
    PRIMARY KEY (room_id, date)
);
```

### Elasticsearch Schema

```json
{
  "mappings": {
    "properties": {
      "hotel_id": { "type": "long" },
      "name": { "type": "text", "analyzer": "standard" },
      "city": { "type": "keyword" },
      "location": { "type": "geo_point" },
      "star_rating": { "type": "float" },
      "average_price": { "type": "float" },
      "amenities": { "type": "keyword" },
      "available": { "type": "boolean" }
    }
  }
}
```

### Key Design Decisions

1. **Rate Limiting**
   - **Algorithm**: Token bucket
   - **Limit**: 100 requests/minute per IP
   - **Implementation**: Redis with sliding window
   ```python
   key = f"rate_limit:{ip_address}:{minute}"
   count = redis.incr(key)
   redis.expire(key, 60)
   if count > 100: reject_request()
   ```

2. **Caching Strategy**
   - **L1 Cache (Redis)**: Popular searches, TTL 5 minutes
   - **L2 Cache**: Elasticsearch query cache
   - **Cache key**: `search:{city}:{checkin}:{checkout}:{guests}:{price_range}`
   - **Invalidation**: On hotel update, invalidate related keys

3. **Scaling**
   - **Read Replicas**: PostgreSQL for read-heavy operations
   - **ES Cluster**: 3 master, 5 data nodes with sharding
   - **Application**: Horizontal pod autoscaling (HPA) on Kubernetes
   - **Database Partitioning**: Hotels table partitioned by country

4. **Add/Remove Hotel Workflow**
   ```
   Add Hotel:
   1. Write to PostgreSQL (primary)
   2. Async index to Elasticsearch (via Kafka)
   3. Invalidate cache entries
   
   Remove Hotel:
   1. Soft delete in PostgreSQL (is_active = false)
   2. Update ES index
   3. Invalidate cache
   ```

5. **Trade-offs**
   - **Eventual Consistency**: ES lag (1-2 seconds) vs real-time accuracy
     - *Choice*: Acceptable for search, critical for booking
   - **Cache Staleness**: 5-min TTL vs fresh data
     - *Choice*: Reduce DB load, acceptable for search
   - **Denormalization**: ES vs normalized DB
     - *Choice*: Duplicate data for faster search

### API Endpoints

```
GET  /api/v1/hotels/search?city=Mumbai&checkin=2026-02-01&checkout=2026-02-05
POST /api/v1/hotels (Add hotel)
PUT  /api/v1/hotels/:id (Update hotel)
DELETE /api/v1/hotels/:id (Soft delete)
GET  /api/v1/hotels/:id/availability
```

---

## 3. Log/Media Storage System

### Requirements
- Accept data from multiple sources (API, CSV, events)
- Store logs and media efficiently
- Queryable and scalable

### Architecture Diagram

```
┌─────────────┬──────────────┬────────────────┐
│  REST API   │  CSV Upload  │  Event Stream  │
└──────┬──────┴──────┬───────┴────────┬───────┘
       │             │                │
       └─────────────┼────────────────┘
                     │
            ┌────────▼─────────┐
            │  Ingestion Layer │
            │  (API Gateway +  │
            │   Lambda/Service)│
            └────────┬─────────┘
                     │
            ┌────────▼─────────┐
            │   Kafka/Kinesis  │
            │   (Buffer)       │
            └────────┬─────────┘
                     │
         ┌───────────┼───────────┐
         │           │           │
    ┌────▼────┐ ┌───▼────┐ ┌────▼─────┐
    │ Logs    │ │ Images │ │  Videos  │
    │ (S3 +   │ │ (S3 +  │ │  (S3 +   │
    │  Athena)│ │  CDN)  │ │   CDN)   │
    └────┬────┘ └───┬────┘ └────┬─────┘
         │          │           │
    ┌────▼──────────▼───────────▼────┐
    │     Metadata DB (DynamoDB)     │
    │     - file_id, type, location  │
    │     - timestamp, source        │
    └────────────────────────────────┘
```

### Data Schema

**DynamoDB Metadata**
```json
{
  "file_id": "uuid-v4",
  "file_type": "log|image|video",
  "source": "api|csv|event",
  "s3_bucket": "bucket-name",
  "s3_key": "2026/01/29/uuid.log",
  "size_bytes": 1024000,
  "content_type": "application/json",
  "uploaded_at": "2026-01-29T10:00:00Z",
  "tags": ["production", "error"],
  "metadata": {
    "user_id": "123",
    "ip_address": "1.2.3.4"
  }
}
```

**S3 Bucket Structure**
```
logs/
  ├── 2026/
  │   ├── 01/
  │   │   ├── 29/
  │   │   │   ├── application-{uuid}.log
  │   │   │   └── system-{uuid}.log

media/
  ├── images/
  │   ├── 2026/01/29/
  │   │   └── {uuid}.jpg
  ├── videos/
      ├── 2026/01/29/
          └── {uuid}.mp4
```

### Ingestion Workflow

1. **REST API Ingestion**
```python
POST /api/v1/upload
{
  "type": "log",
  "content": "...",
  "tags": ["error", "production"]
}

# Process:
1. Validate schema
2. Generate file_id
3. Push to Kafka topic
4. Return file_id to client
```

2. **CSV Upload**
```python
POST /api/v1/bulk-upload
Content-Type: multipart/form-data
file: logs.csv

# Process:
1. Parse CSV in streaming mode
2. Validate each row
3. Batch write to Kafka (1000 records/batch)
```

3. **Event Stream**
```python
# Direct Kafka producer from applications
kafka.send(
  topic='log-events',
  value={'log': '...', 'timestamp': '...'}
)
```

### Key Design Decisions

1. **Multi-source Ingestion**
   - Unified Kafka topic per data type
   - Schema validation at ingestion layer (Avro/JSON Schema)
   - Dead letter queue for failed validations

2. **Storage Strategy**
   - **Logs**: S3 Standard → S3 IA (30 days) → Glacier (90 days)
   - **Images**: S3 Standard + CloudFront CDN
   - **Videos**: S3 + CloudFront with streaming protocols (HLS)

3. **Partitioning**
   - By date: `YYYY/MM/DD/`
   - By source type: `api/`, `csv/`, `event/`
   - Enables efficient Athena queries

4. **Schema Enforcement**
```json
{
  "type": "object",
  "required": ["file_type", "source", "content"],
  "properties": {
    "file_type": { "enum": ["log", "image", "video"] },
    "source": { "enum": ["api", "csv", "event"] },
    "content": { "type": "string" }
  }
}
```

5. **Query Layer**
   - **Athena** for log analysis
   - **DynamoDB** for metadata queries
   - **Example query**:
   ```sql
   SELECT * FROM logs
   WHERE year='2026' AND month='01' AND day='29'
   AND json_extract(data, '$.level') = 'ERROR'
   ```

### Processing Pipeline

```
Kafka Consumer → Lambda/Fargate:
1. Read message from Kafka
2. Validate schema
3. Upload to S3
4. Write metadata to DynamoDB
5. Commit offset
```

---

## 4. Flight Search System

### Requirements
- Integrate with 3rd party flight APIs
- Handle metered API costs
- Dynamic real-time price changes
- Fast search results

### Architecture Diagram

```
┌────────────┐
│   Client   │
└─────┬──────┘
      │
┌─────▼──────────────────┐
│   BFF (Backend for     │
│   Frontend)            │
└─────┬──────────────────┘
      │
┌─────▼─────────────────────────────┐
│  Aggregator Service               │
│  (Fan-out to providers)           │
└─────┬─────────────────────────────┘
      │
      ├───────────┬────────────┬──────────┐
      │           │            │          │
┌─────▼──────┐ ┌──▼────┐ ┌────▼───┐ ┌────▼───┐
│ Provider 1 │ │ Prov 2│ │ Prov 3 │ │ Cache  │
│ (Amadeus)  │ │ (API) │ │ (API)  │ │ (Redis)│
└────────────┘ └───────┘ └────────┘ └────┬───┘
                                          │
┌─────────────────────────────────────────▼──┐
│  Price History DB (Time-series DB/         │
│  InfluxDB) - for analytics                 │
└────────────────────────────────────────────┘
```

### Data Schema

**Cache Schema (Redis)**
```json
Key: "flight:{origin}:{dest}:{date}:{class}"
Value: {
  "flights": [
    {
      "flight_id": "AI-101",
      "airline": "Air India",
      "price": 5500,
      "currency": "INR",
      "departure": "2026-02-01T10:00:00Z",
      "arrival": "2026-02-01T12:30:00Z",
      "stops": 0,
      "cached_at": "2026-01-29T10:00:00Z"
    }
  ]
}
TTL: 120 seconds (2 minutes for popular routes)
TTL: 300 seconds (5 minutes for less popular routes)
```

**Price History (InfluxDB)**
```
measurement: flight_prices
tags: flight_id, origin, destination, airline
fields: price, currency, seats_available
timestamp: time of recording
```

### API Integration

**Provider API Call**
```python
async def fetch_from_provider(provider, search_params):
    # Circuit breaker pattern
    if circuit_breaker.is_open(provider):
        return None
    
    try:
        response = await http_client.post(
            provider.api_url,
            json=search_params,
            timeout=3.0  # 3 second timeout
        )
        
        # Track API usage for metering
        await quota_manager.increment(provider)
        
        return response.json()
    except Exception as e:
        circuit_breaker.record_failure(provider)
        return None
```

### Key Design Decisions

1. **3rd Party API Cost Management**
   - **Aggressive caching**: 
     - Popular routes (>100 searches/hour): 2 min TTL
     - Medium routes: 5 min TTL
     - Rare routes: Direct API call, cache 10 min
   - **Quota management per provider**:
   ```python
   daily_quota = {
     'amadeus': 10000,
     'sabre': 5000,
     'travelport': 8000
   }
   ```
   - **Circuit breaker**: Auto-disable provider after 5 consecutive failures

2. **Dynamic Real-time Pricing**
   - **Initial search**: Cached results returned immediately
   - **WebSocket connection**: Post-search for real-time updates
   ```javascript
   ws.send({
     action: 'subscribe',
     flight_ids: ['AI-101', 'UK-202']
   })
   ```
   - **Price update frequency**: Every 30 seconds for subscribed flights

3. **Metered API Optimization**
   - **Batch requests**: Combine multiple searches when possible
   - **Smart routing**: Route to cheapest provider first
   - **Fallback chain**: Primary → Secondary → Tertiary
   - **Request coalescing**: Merge identical concurrent requests

4. **Search Optimization**
   ```python
   async def search_flights(params):
       # Check cache first
       cached = await redis.get(cache_key)
       if cached and not expired(cached):
           return cached
       
       # Parallel API calls with timeout
       tasks = [
           fetch_from_provider(p, params) 
           for p in active_providers
       ]
       results = await asyncio.gather(*tasks, timeout=3.0)
       
       # Merge and deduplicate
       merged = merge_results(results)
       
       # Cache result
       await redis.setex(cache_key, ttl, merged)
       
       return merged
   ```

5. **Partial Results Strategy**
   - Return results as they arrive (streaming)
   - Don't wait for all providers
   - Show loading indicators for pending providers

### API Endpoints

```
POST /api/v1/flights/search
{
  "origin": "DEL",
  "destination": "BOM",
  "date": "2026-02-01",
  "passengers": 2,
  "class": "economy"
}

GET /api/v1/flights/:id/price-history

WebSocket: /ws/flights/price-updates
```

---

## 5. YouTube Design

### Requirements
- Registered users can upload videos
- Any user can search and view
- Scalable video streaming
- Fast search

### Architecture Diagram

```
┌──────────────┐
│    Client    │
└──────┬───────┘
       │
┌──────▼────────────────────────┐
│   CDN (CloudFront/Akamai)     │
│   (Video delivery)            │
└──────┬────────────────────────┘
       │
┌──────▼────────────────────────┐
│   Load Balancer               │
└──────┬────────────────────────┘
       │
       ├──────────────────┬───────────────┐
       │                  │               │
┌──────▼───────┐  ┌───────▼────┐  ┌──────▼─────┐
│ Upload Svc   │  │ Search Svc │  │ Stream Svc │
└──────┬───────┘  └───────┬────┘  └──────┬─────┘
       │                  │               │
┌──────▼───────┐  ┌───────▼────────┐     │
│ S3 (Raw)     │  │ Elasticsearch  │     │
└──────┬───────┘  └────────────────┘     │
       │                                  │
┌──────▼────────────────┐         ┌──────▼─────────┐
│  Transcoding Queue    │         │  S3 (Encoded)  │
│  (SQS/Kafka)          │         │  - 1080p       │
└──────┬────────────────┘         │  - 720p        │
       │                          │  - 480p        │
┌──────▼────────────────┐         │  - 360p        │
│  Transcoding Workers  │         └────────────────┘
│  (FFmpeg on K8s)      │
└───────────────────────┘

┌──────────────────────────────────┐
│   Metadata DB (PostgreSQL)       │
│   - video_id, title, user_id     │
│   - views, likes, upload_time    │
└──────────────────────────────────┘

┌──────────────────────────────────┐
│   Analytics DB (Cassandra)       │
│   - view events, watch time      │
└──────────────────────────────────┘
```

### Data Schema

**Videos Table (PostgreSQL)**
```sql
CREATE TABLE videos (
    video_id BIGSERIAL PRIMARY KEY,
    user_id BIGINT NOT NULL,
    title VARCHAR(255) NOT NULL,
    description TEXT,
    duration_seconds INT,
    upload_time TIMESTAMP DEFAULT NOW(),
    status VARCHAR(20), -- 'processing', 'ready', 'failed'
    view_count BIGINT DEFAULT 0,
    like_count BIGINT DEFAULT 0,
    thumbnail_url TEXT,
    raw_video_url TEXT,
    encoded_video_urls JSONB, -- {1080p: url, 720p: url, ...}
    is_public BOOLEAN DEFAULT true
);

CREATE INDEX idx_videos_user ON videos(user_id);
CREATE INDEX idx_videos_upload_time ON videos(upload_time DESC);
CREATE INDEX idx_videos_views ON videos(view_count DESC);
```

**Elasticsearch Index**
```json
{
  "mappings": {
    "properties": {
      "video_id": { "type": "long" },
      "title": { 
        "type": "text", 
        "analyzer": "standard",
        "fields": {
          "autocomplete": { 
            "type": "text", 
            "analyzer": "autocomplete" 
          }
        }
      },
      "description": { "type": "text" },
      "tags": { "type": "keyword" },
      "upload_time": { "type": "date" },
      "view_count": { "type": "long" },
      "duration": { "type": "integer" }
    }
  }
}
```

**View Events (Cassandra)**
```cql
CREATE TABLE view_events (
    video_id BIGINT,
    user_id BIGINT,
    viewed_at TIMESTAMP,
    watch_duration_seconds INT,
    quality VARCHAR(10),
    PRIMARY KEY (video_id, viewed_at)
) WITH CLUSTERING ORDER BY (viewed_at DESC);
```

### Upload Workflow

```
1. Client initiates upload
   POST /api/v1/videos/upload-init
   Response: { upload_id, presigned_url }

2. Client uploads to S3 via presigned URL
   PUT {presigned_url}
   (Multipart upload for large files)

3. S3 triggers Lambda/Event
   → Insert metadata in DB (status: 'processing')
   → Push message to transcoding queue

4. Transcoding workers consume queue
   → Download from S3
   → Transcode using FFmpeg:
     - 1080p (H.264, 5 Mbps)
     - 720p (H.264, 2.5 Mbps)
     - 480p (H.264, 1 Mbps)
     - 360p (H.264, 500 Kbps)
   → Upload encoded files to S3
   → Generate thumbnail
   → Update DB (status: 'ready')
   → Index to Elasticsearch

5. Notify user (email/push notification)
```

### Streaming Workflow

```
1. User requests video
   GET /api/v1/videos/{video_id}/stream

2. Check authorization
   - If public: allow
   - If private: verify user permission

3. Return manifest file (HLS/DASH)
   {
     "manifest_url": "https://cdn.../video.m3u8",
     "qualities": ["1080p", "720p", "480p", "360p"]
   }

4. Client plays via CDN
   - Adaptive bitrate streaming
   - CDN serves 90%+ requests
   - Origin (S3) only for cache misses

5. Log view event
   → Kafka → Cassandra
   → Update view_count (async batch job)
```

### Search Implementation

**Autocomplete**
```
GET /api/v1/search/suggest?q=pyt

Response:
{
  "suggestions": [
    "python tutorial",
    "python for beginners",
    "pytorch deep learning"
  ]
}
```

**Full Search**
```
GET /api/v1/search?q=python&sort=views&filter=upload_date:last_week

Elasticsearch Query:
{
  "query": {
    "bool": {
      "must": [
        { "match": { "title": "python" } }
      ],
      "filter": [
        { "range": { "upload_time": { "gte": "now-7d" } } }
      ]
    }
  },
  "sort": [{ "view_count": "desc" }]
}
```

### Key Design Decisions

1. **Upload Strategy**
   - **Chunked upload**: 5 MB chunks for large files
   - **Presigned URLs**: Offload upload to S3 directly
   - **Async processing**: Don't block user, process in background

2. **Transcoding**
   - **Horizontal scaling**: K8s pods with autoscaling
   - **Priority queue**: Paid users get higher priority
   - **Failed jobs**: Retry 3 times, then mark as failed

3. **Streaming**
   - **Adaptive bitrate**: HLS/DASH protocols
   - **CDN**: 90%+ hit rate, reduce origin load
   - **DRM**: Encrypted streams for premium content

4. **Search**
   - **Elasticsearch**: Fast full-text search
   - **Autocomplete**: Edge n-gram analyzer
   - **Ranking**: Combine relevance + popularity

5. **Scalability**
   - **Database**: Read replicas for metadata
   - **CDN**: Global edge locations
   - **Caching**: Redis for hot videos metadata

### API Endpoints

```
POST   /api/v1/videos/upload-init
GET    /api/v1/videos/:id/stream
GET    /api/v1/search?q=query
GET    /api/v1/search/suggest?q=prefix
POST   /api/v1/videos/:id/like
POST   /api/v1/videos/:id/comment
GET    /api/v1/videos/:id/recommendations
```

---

## 6. Hotel Booking - Proximity Search

### Requirements
- Search hotels near a location
- Fast proximity-based queries
- Filter by amenities, price, rating
- High availability

### Architecture Diagram

```
┌────────────┐
│   Client   │
└─────┬──────┘
      │
┌─────▼──────────────────────┐
│  Search Service            │
│  (Geo-spatial queries)     │
└─────┬──────────────────────┘
      │
      ├────────────────┬──────────────┐
      │                │              │
┌─────▼─────┐  ┌───────▼──────┐  ┌───▼────────┐
│ Redis     │  │ PostgreSQL + │  │ Elasticsearch│
│ Geo Cache │  │ PostGIS      │  │ (Geo-point) │
└───────────┘  └───────┬──────┘  └────────────┘
                       │
              ┌────────▼─────────┐
              │ Hotels Table     │
              │ - lat, long      │
              │ - geohash        │
              └──────────────────┘
```

### Data Schema

**Hotels Table (PostgreSQL + PostGIS)**
```sql
CREATE EXTENSION postgis;

CREATE TABLE hotels (
    hotel_id BIGSERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    address TEXT,
    location GEOGRAPHY(POINT, 4326), -- lat/long
    geohash VARCHAR(12),
    star_rating DECIMAL(2,1),
    price_range VARCHAR(10), -- '$', '$$', '$$$'
    amenities TEXT[],
    average_rating DECIMAL(3,2),
    total_rooms INT,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Spatial index
CREATE INDEX idx_hotels_location ON hotels USING GIST(location);
CREATE INDEX idx_hotels_geohash ON hotels(geohash);
CREATE INDEX idx_hotels_price ON hotels(price_range);
```

**Geohash Explanation**
```
Geohash: Hierarchical spatial index
- 1 char: ~5,000 km
- 2 char: ~1,250 km
- 3 char: ~156 km
- 4 char: ~39 km
- 5 char: ~4.9 km
- 6 char: ~1.2 km (used for proximity search)
- 7 char: ~153 m

Example:
Mumbai (19.0760, 72.8777) → geohash: "te7tc9"
```

### Proximity Search Algorithm

**1. Geohash Prefix Search (Fast)**
```python
def search_nearby_hotels(lat, lng, radius_km):
    # Generate geohash for search location
    center_geohash = geohash.encode(lat, lng, precision=6)
    
    # Get neighboring geohashes
    neighbors = geohash.neighbors(center_geohash)
    all_geohashes = [center_geohash] + neighbors
    
    # Query with geohash prefix
    query = """
        SELECT * FROM hotels 
        WHERE geohash LIKE ANY(%s)
        AND ST_DWithin(
            location, 
            ST_Point(%s, %s)::geography, 
            %s
        )
        ORDER BY ST_Distance(location, ST_Point(%s, %s)::geography)
        LIMIT 20
    """
    
    geohash_patterns = [gh + '%' for gh in all_geohashes]
    results = execute(query, [
        geohash_patterns, 
        lng, lat, 
        radius_km * 1000,  # meters
        lng, lat
    ])
    
    return results
```

**2. Exact Distance Calculation (Haversine)**
```python
def haversine_distance(lat1, lon1, lat2, lon2):
    R = 6371  # Earth's radius in km
    
    dLat = math.radians(lat2 - lat1)
    dLon = math.radians(lon2 - lon1)
    
    a = (math.sin(dLat/2) * math.sin(dLat/2) +
         math.cos(math.radians(lat1)) * 
         math.cos(math.radians(lat2)) *
         math.sin(dLon/2) * math.sin(dLon/2))
    
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))
    distance = R * c
    
    return distance
```

**3. Redis Geo Commands (Cache)**
```python
# Add hotels to Redis
redis.geoadd(
    'hotels:geo',
    (lng, lat, hotel_id)
)

# Search nearby
results = redis.georadius(
    'hotels:geo',
    lng, lat,
    5,  # radius in km
    unit='km',
    withdist=True,
    sort='ASC',
    count=20
)
```

### Search Query with Filters

```sql
-- Full query with filters
SELECT 
    h.hotel_id,
    h.name,
    h.star_rating,
    h.average_rating,
    ST_Distance(h.location, ST_Point($1, $2)::geography) / 1000 AS distance_km
FROM hotels h
WHERE 
    h.geohash LIKE ANY($3)  -- Geohash prefix filter
    AND ST_DWithin(h.location, ST_Point($1, $2)::geography, $4)  -- Exact distance
    AND h.star_rating >= $5  -- Rating filter
    AND h.price_range = ANY($6)  -- Price filter
    AND h.amenities @> $7  -- Amenities filter (array contains)
    AND h.average_rating >= $8  -- Review rating
ORDER BY distance_km ASC
LIMIT 20;
```

### Elasticsearch Geo Query

```json
{
  "query": {
    "bool": {
      "must": [
        {
          "geo_distance": {
            "distance": "5km",
            "location": {
              "lat": 19.0760,
              "lon": 72.8777
            }
          }
        }
      ],
      "filter": [
        { "range": { "star_rating": { "gte": 4 } } },
        { "terms": { "amenities": ["wifi", "pool"] } }
      ]
    }
  },
  "sort": [
    {
      "_geo_distance": {
        "location": { "lat": 19.0760, "lon": 72.8777 },
        "order": "asc",
        "unit": "km"
      }
    }
  ]
}
```

### Caching Strategy

**Cache Key Design**
```
geo:search:{lat}:{lng}:{radius}:{filters_hash}
TTL: 300 seconds (5 minutes)
```

**Cache Invalidation**
- On hotel update/delete: Invalidate all geo cache keys
- Use Redis SCAN to find matching keys
- Alternatively: Tag-based invalidation

### Key Design Decisions

1. **Geohash for Initial Filtering**
   - Reduces search space by 90%+
   - O(1) prefix lookup
   - Then exact distance calculation on smaller set

2. **Three-tier Search**
   - **L1**: Redis Geo (sub-millisecond)
   - **L2**: Elasticsearch (10-50ms)
   - **L3**: PostgreSQL + PostGIS (100-200ms)

3. **Index Strategy**
   - GIST index on geography column
   - B-tree index on geohash
   - Composite index: (geohash, star_rating, price_range)

4. **Distance Calculation**
   - Geohash for coarse filtering
   - PostGIS ST_Distance for exact distance
   - Haversine in application layer for very hot paths

5. **Scaling**
   - Partition by region (country/state)
   - Read replicas for each region
   - CDN for static hotel images

### API Endpoints

```
GET /api/v1/hotels/nearby?lat=19.0760&lng=72.8777&radius=5
  &min_rating=4&amenities=wifi,pool&price=$$

GET /api/v1/hotels/:id/details

POST /api/v1/hotels/:id/book
```

---

## 7. Distributed Scheduler (500M URLs)

### Requirements
- Schedule URL crawls based on frequency
- Handle 500 million URLs
- Distributed and fault-tolerant
- Support different crawl frequencies

### Architecture Diagram

```
┌──────────────────────────────┐
│   RDBMS (Sharded)            │
│   - url, frequency, last_run │
│   Shard by: hash(url) % N    │
└──────────┬───────────────────┘
           │
┌──────────▼────────────────────┐
│  Scheduler Coordinator        │
│  (Leader election: Zookeeper) │
└──────────┬────────────────────┘
           │
    ┌──────┼──────┬──────┐
    │      │      │      │
┌───▼──┐ ┌─▼──┐ ┌▼───┐ ┌▼───┐
│ Wkr 1│ │Wkr2│ │Wkr3│ │WkrN│
└───┬──┘ └─┬──┘ └┬───┘ └┬───┘
    │      │     │      │
    └──────┴─────┴──────┘
           │
    ┌──────▼──────┐
    │ Task Queue  │
    │ (Kafka)     │
    └─────────────┘
```

### Data Schema

**URLs Table (Sharded PostgreSQL)**
```sql
CREATE TABLE urls (
    url_id BIGSERIAL,
    url TEXT NOT NULL,
    url_hash VARCHAR(64), -- SHA-256 hash for partitioning
    shard_id INT, -- hash(url) % num_shards
    frequency VARCHAR(20), -- 'high', 'medium', 'low'
    interval_seconds INT, -- 300, 3600, 86400
    last_crawled_at TIMESTAMP,
    next_crawl_at TIMESTAMP,
    priority INT DEFAULT 0,
    retry_count INT DEFAULT 0,
    status VARCHAR(20), -- 'active', 'paused', 'failed'
    created_at TIMESTAMP DEFAULT NOW(),
    PRIMARY KEY (shard_id, url_id)
);

CREATE INDEX idx_urls_next_crawl ON urls(shard_id, next_crawl_at)
  WHERE status = 'active';
CREATE INDEX idx_urls_frequency ON urls(frequency);
```

**Partitioning Strategy**
```sql
-- Partition by shard_id (1000 shards)
CREATE TABLE urls_shard_0 PARTITION OF urls
  FOR VALUES FROM (0) TO (1);

CREATE TABLE urls_shard_1 PARTITION OF urls
  FOR VALUES FROM (1) TO (2);
  
-- ... 998 more partitions
```

**Frequency Buckets**
```python
FREQUENCY_CONFIG = {
    'high': {
        'interval': 300,      # 5 minutes
        'examples': ['news sites', 'stock prices']
    },
    'medium': {
        'interval': 3600,     # 1 hour
        'examples': ['blogs', 'forums']
    },
    'low': {
        'interval': 86400,    # 1 day
        'examples': ['static pages', 'archives']
    }
}
```

### Scheduler Coordinator

**Leader Election (Zookeeper)**
```python
class SchedulerCoordinator:
    def __init__(self, zk_client):
        self.zk = zk_client
        self.is_leader = False
        
    def elect_leader(self):
        # Create ephemeral node
        try:
            self.zk.create('/scheduler/leader', 
                          ephemeral=True, 
                          value=self.worker_id)
            self.is_leader = True
        except NodeExistsError:
            # Another worker is leader
            self.watch_leader()
    
    def assign_shards(self):
        if not self.is_leader:
            return
        
        # Get active workers
        workers = self.zk.get_children('/scheduler/workers')
        
        # Distribute 1000 shards among workers
        shards_per_worker = 1000 // len(workers)
        
        for i, worker in enumerate(workers):
            start_shard = i * shards_per_worker
            end_shard = start_shard + shards_per_worker
            
            self.zk.set(f'/scheduler/workers/{worker}/shards',
                       json.dumps({
                           'start': start_shard,
                           'end': end_shard
                       }))
```

### Worker Implementation

**Worker Process**
```python
class SchedulerWorker:
    def __init__(self, worker_id, db_pool, kafka_producer):
        self.worker_id = worker_id
        self.db = db_pool
        self.kafka = kafka_producer
        self.assigned_shards = []
        
    async def run(self):
        while True:
            # Get assigned shards from Zookeeper
            self.assigned_shards = await self.get_assigned_shards()
            
            # Query URLs due for crawling
            urls_to_crawl = await self.fetch_due_urls()
            
            # Push to Kafka
            for url in urls_to_crawl:
                await self.kafka.send('crawl-tasks', {
                    'url': url['url'],
                    'url_id': url['url_id'],
                    'priority': url['priority']
                })
            
            # Update next_crawl_at
            await self.update_next_crawl_times(urls_to_crawl)
            
            # Sleep for 1 minute
            await asyncio.sleep(60)
    
    async def fetch_due_urls(self):
        query = """
            SELECT url_id, url, frequency, priority
            FROM urls
            WHERE shard_id = ANY(%s)
              AND next_crawl_at <= NOW()
              AND status = 'active'
            ORDER BY priority DESC, next_crawl_at ASC
            LIMIT 10000
        """
        return await self.db.fetch(query, self.assigned_shards)
    
    async def update_next_crawl_times(self, urls):
        # Batch update
        updates = [
            (url['url_id'], 
             datetime.now() + timedelta(seconds=url['interval']))
            for url in urls
        ]
        
        query = """
            UPDATE urls
            SET next_crawl_at = data.next_crawl
            FROM (VALUES %s) AS data(url_id, next_crawl)
            WHERE urls.url_id = data.url_id
        """
        await self.db.execute_batch(query, updates)
```

### Key Design Decisions

1. **Partitioning Strategy**
   - **Consistent hashing**: `shard_id = hash(url) % 1000`
   - Each worker handles 100-200 shards (for 5-10 workers)
   - Even distribution of load

2. **Frequency-based Scheduling**
   - High frequency: Scanned every 1 minute
   - Medium: Every 5 minutes
   - Low: Every 15 minutes
   - Separate queues for each frequency tier

3. **Fault Tolerance**
   - **Leader election**: Zookeeper for coordinator
   - **Worker failure**: Reassign shards to healthy workers
   - **Database failure**: Use read replicas, retry logic

4. **Scaling**
   - Horizontal: Add more workers
   - Vertical: Increase shard count (1000 → 2000)
   - Rebalancing: Triggered manually or on worker join/leave

5. **Optimization**
   - **Batch queries**: Fetch 10,000 URLs per iteration
   - **Batch updates**: Update next_crawl_at in batches
   - **Indexing**: Composite index on (shard_id, next_crawl_at)

6. **Priority Handling**
   ```python
   # High priority URLs get scheduled first
   ORDER BY priority DESC, next_crawl_at ASC
   ```

### Shard Assignment Example

```
Workers: 10
Shards: 1000
Shards per worker: 100

Worker 1: Shards 0-99
Worker 2: Shards 100-199
Worker 3: Shards 200-299
...
Worker 10: Shards 900-999

On worker failure (Worker 3):
- Shards 200-299 reassigned to Worker 1 and Worker 2
- Each takes 50 shards temporarily
```

### Monitoring

**Metrics to Track**
- URLs scheduled per minute
- Average scheduling latency
- Queue depth (Kafka lag)
- Worker health (via Zookeeper heartbeat)
- Shard distribution balance

---

## 8. Payment Gateway System

### Requirements
- High scalability
- Exactly-once message processing (Kafka)
- Transaction integrity
- Support multiple payment methods

### Architecture Diagram

```
┌─────────────┐
│   Merchant  │
└──────┬──────┘
       │
┌──────▼──────────────────────┐
│   API Gateway + Auth        │
└──────┬──────────────────────┘
       │
┌──────▼──────────────────────┐
│   Payment Service           │
│   (Idempotency check)       │
└──────┬──────────────────────┘
       │
┌──────▼───────────────────────┐
│   Kafka (Exactly-once)       │
│   - enable.idempotence=true  │
│   - transactional.id=set     │
└──────┬───────────────────────┘
       │
┌──────▼───────────────────────┐
│   Payment Processor          │
│   (Consume + DB Write)       │
└──────┬───────────────────────┘
       │
┌──────▼────────────────────────┐
│   PostgreSQL                  │
│   - txn_id (unique)           │
│   - status, amount, merchant  │
└───────────────────────────────┘
       │
┌──────▼────────────────────────┐
│   Payment Providers           │
│   (Stripe, Razorpay, PayPal)  │
└───────────────────────────────┘
```

### Data Schema

**Transactions Table**
```sql
CREATE TABLE transactions (
    transaction_id UUID PRIMARY KEY,
    merchant_id BIGINT NOT NULL,
    customer_id BIGINT,
    idempotency_key VARCHAR(255) UNIQUE NOT NULL,
    amount DECIMAL(10, 2) NOT NULL,
    currency VARCHAR(3) NOT NULL,
    payment_method VARCHAR(50), -- 'card', 'upi', 'wallet'
    provider VARCHAR(50), -- 'stripe', 'razorpay'
    provider_txn_id VARCHAR(255),
    status VARCHAR(20), -- 'pending', 'processing', 'success', 'failed'
    metadata JSONB,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_txn_merchant ON transactions(merchant_id, created_at DESC);
CREATE INDEX idx_txn_status ON transactions(status);
CREATE INDEX idx_txn_idempotency ON transactions(idempotency_key);
```

**Kafka Events Schema**
```json
{
  "event_id": "uuid",
  "event_type": "payment.initiated",
  "transaction_id": "uuid",
  "merchant_id": 12345,
  "amount": 100.00,
  "currency": "INR",
  "idempotency_key": "unique-key-from-merchant",
  "timestamp": "2026-01-29T10:00:00Z",
  "metadata": {}
}
```

### Exactly-Once Processing in Kafka

**Producer Configuration**
```python
producer_config = {
    'bootstrap.servers': 'kafka:9092',
    'enable.idempotence': True,  # Critical for exactly-once
    'acks': 'all',  # Wait for all replicas
    'retries': 10,
    'max.in.flight.requests.per.connection': 5,
    'transactional.id': 'payment-producer-1'  # Unique per instance
}

producer = KafkaProducer(**producer_config)

# Initialize transactions
producer.init_transactions()

try:
    producer.begin_transaction()
    
    # Send payment event
    producer.send('payment-events', payment_event)
    
    # Commit transaction
    producer.commit_transaction()
except Exception as e:
    producer.abort_transaction()
    raise
```

**Consumer Configuration**
```python
consumer_config = {
    'bootstrap.servers': 'kafka:9092',
    'group.id': 'payment-processors',
    'enable.auto.commit': False,  # Manual commit for exactly-once
    'isolation.level': 'read_committed',  # Only read committed messages
    'auto.offset.reset': 'earliest'
}

consumer = KafkaConsumer(**consumer_config)
consumer.subscribe(['payment-events'])

for message in consumer:
    try:
        # Process payment
        result = process_payment(message.value)
        
        # Write to database in same transaction
        with db.transaction():
            save_transaction(result)
            
            # Commit Kafka offset
            consumer.commit()
    except Exception as e:
        logger.error(f"Processing failed: {e}")
        # Don't commit offset, message will be reprocessed
```

### Idempotency Implementation

**API Layer Idempotency**
```python
@app.post("/api/v1/payments")
async def create_payment(payment: PaymentRequest):
    idempotency_key = payment.idempotency_key
    
    # Check if already processed
    existing = await db.fetch_one(
        "SELECT * FROM transactions WHERE idempotency_key = $1",
        idempotency_key
    )
    
    if existing:
        # Return cached response
        return {
            "transaction_id": existing['transaction_id'],
            "status": existing['status'],
            "cached": True
        }
    
    # Create new transaction
    txn_id = str(uuid.uuid4())
    
    await db.execute("""
        INSERT INTO transactions 
        (transaction_id, idempotency_key, merchant_id, amount, status)
        VALUES ($1, $2, $3, $4, 'pending')
    """, txn_id, idempotency_key, payment.merchant_id, payment.amount)
    
    # Publish to Kafka
    await kafka_producer.send({
        'transaction_id': txn_id,
        'idempotency_key': idempotency_key,
        ...
    })
    
    return {"transaction_id": txn_id, "status": "pending"}
```

### Payment Processing Flow

```
1. Merchant initiates payment
   POST /api/v1/payments
   Headers: X-Idempotency-Key: unique-key
   Body: { amount, currency, customer_id, payment_method }

2. Payment Service
   - Validate idempotency key
   - Create transaction record (status: pending)
   - Publish to Kafka (transactional)

3. Kafka Consumer (Payment Processor)
   - Read message
   - Call payment provider API (Stripe/Razorpay)
   - Update transaction status
   - Commit Kafka offset
   
4. Webhook from provider (for async confirmation)
   - Verify signature
   - Update final status
   - Trigger merchant webhook

5. Merchant receives webhook
   POST {merchant_webhook_url}
   Body: { transaction_id, status, amount }
```

### Handling Failures

**1. Network Failure During Provider Call**
```python
async def process_payment(txn):
    max_retries = 3
    for attempt in range(max_retries):
        try:
            # Call payment provider
            result = await payment_provider.charge(
                amount=txn['amount'],
                token=txn['payment_token']
            )
            
            # Update DB
            await db.execute("""
                UPDATE transactions 
                SET status = 'success', 
                    provider_txn_id = $1
                WHERE transaction_id = $2
            """, result['id'], txn['transaction_id'])
            
            return result
            
        except NetworkError:
            if attempt < max_retries - 1:
                await asyncio.sleep(2 ** attempt)  # Exponential backoff
                continue
            else:
                # Mark as failed after retries
                await db.execute("""
                    UPDATE transactions 
                    SET status = 'failed'
                    WHERE transaction_id = $1
                """, txn['transaction_id'])
                raise
```

**2. Duplicate Message Handling**
```python
# Transaction ID is unique, INSERT will fail on duplicate
try:
    await db.execute("""
        INSERT INTO transactions (transaction_id, ...)
        VALUES ($1, ...)
    """, txn_id, ...)
except UniqueViolationError:
    # Already processed, skip
    logger.info(f"Duplicate transaction: {txn_id}")
    return
```

**3. Consumer Crash**
- Kafka rebalances partition to another consumer
- New consumer processes from last committed offset
- Exactly-once semantics ensure no duplicates

### Key Design Decisions

1. **Exactly-Once in Kafka**
   - **Producer**: Idempotent producer + transactions
   - **Consumer**: Manual offset commit + read_committed
   - **Database**: Transactional writes with offset commit

2. **Idempotency**
   - **Merchant-provided key**: Unique per payment attempt
   - **Database constraint**: UNIQUE on idempotency_key
   - **Cache**: Redis for fast idempotency checks (optional)

3. **Scaling**
   - **Kafka partitions**: Partition by merchant_id for parallelism
   - **Consumer groups**: Scale to number of partitions
   - **Database**: Partition transactions table by merchant_id

4. **Security**
   - **API authentication**: JWT tokens
   - **Webhook signature**: HMAC verification
   - **Encryption**: PCI DSS compliant data handling

5. **Monitoring**
   - Transaction success rate
   - Provider latency
   - Kafka consumer lag
   - Failed transaction alerts

### Scalability Numbers

```
Assumption: 10,000 TPS (transactions per second)

Kafka:
- 100 partitions
- 10 consumers (100 TPS each)
- Replication factor: 3

Database:
- 10 write-optimized instances
- 1000 TPS per instance
- Partitioned by merchant_id

Payment Providers:
- Rate limits: 100 TPS per provider
- Use 3 providers (Stripe, Razorpay, PayPal)
- Load balance based on geography/preference
```

---

## 9. File Storage Service

### Requirements
- Upload and download files
- Access control
- Deduplication
- Scalability

### Architecture Diagram

```
┌──────────────┐
│    Client    │
└──────┬───────┘
       │
┌──────▼────────────────┐
│   API Gateway         │
└──────┬────────────────┘
       │
┌──────▼─────────────────┐
│   Upload/Download Svc  │
│   (Pre-signed URLs)    │
└──────┬─────────────────┘
       │
┌──────▼─────────────────┐
│   S3 / Object Storage  │
│   - Versioning         │
│   - Lifecycle policies │
└──────┬─────────────────┘
       │
┌──────▼──────────────────┐
│   Metadata DB (Dynamo)  │
│   - file_id, owner      │
│   - size, s3_key        │
│   - permissions         │
└─────────────────────────┘
```

### Data Schema

**Metadata (DynamoDB)**
```json
{
  "file_id": "uuid",
  "owner_id": "user-123",
  "file_name": "document.pdf",
  "file_size": 1024000,
  "content_type": "application/pdf",
  "s3_bucket": "files-prod",
  "s3_key": "users/user-123/uuid.pdf",
  "file_hash": "sha256-hash",
  "version": 1,
  "is_public": false,
  "shared_with": ["user-456", "user-789"],
  "permissions": {
    "user-456": "read",
    "user-789": "write"
  },
  "created_at": "2026-01-29T10:00:00Z",
  "updated_at": "2026-01-29T10:00:00Z",
  "expires_at": null
}
```

**S3 Bucket Structure**
```
bucket-name/
├── users/
│   ├── user-123/
│   │   ├── uuid1.pdf
│   │   └── uuid2.jpg
│   └── user-456/
│       └── uuid3.docx
├── shared/
│   └── team-abc/
│       └── uuid4.xlsx
└── public/
    └── uuid5.png
```

### Upload Workflow

**1. Initiate Upload**
```python
POST /api/v1/files/upload-init
{
  "file_name": "document.pdf",
  "file_size": 1024000,
  "content_type": "application/pdf"
}

Response:
{
  "file_id": "uuid",
  "upload_url": "https://s3.../presigned-url",
  "upload_fields": {
    "key": "users/user-123/uuid.pdf",
    "x-amz-algorithm": "...",
    ...
  }
}
```

**2. Client Uploads to S3**
```javascript
// Multipart upload for large files (>5MB)
const upload = new AWS.S3.ManagedUpload({
  params: {
    Bucket: 'files-prod',
    Key: 'users/user-123/uuid.pdf',
    Body: file
  },
  partSize: 5 * 1024 * 1024, // 5MB parts
  queueSize: 4 // 4 parallel uploads
});

upload.promise();
```

**3. Finalize Upload**
```python
POST /api/v1/files/{file_id}/finalize

# Backend:
1. Verify file exists in S3
2. Calculate file hash (SHA-256)
3. Check for duplicates
4. Update metadata in DynamoDB
5. Return file details
```

### Download Workflow

```python
GET /api/v1/files/{file_id}/download

# Backend:
1. Check permissions
2. Generate pre-signed download URL
3. Return URL (valid for 1 hour)

Response:
{
  "download_url": "https://s3.../presigned-download-url",
  "expires_in": 3600
}
```

### Deduplication

**Hash-based Deduplication**
```python
async def upload_file(file_data, user_id):
    # Calculate SHA-256 hash
    file_hash = hashlib.sha256(file_data).hexdigest()
    
    # Check if file with same hash exists
    existing = await db.query(
        "SELECT * FROM files WHERE file_hash = $1",
        file_hash
    )
    
    if existing:
        # Create reference to existing file
        new_file_id = str(uuid.uuid4())
        
        await db.execute("""
            INSERT INTO files (file_id, owner_id, s3_key, file_hash, ...)
            VALUES ($1, $2, $3, $4, ...)
        """, new_file_id, user_id, existing['s3_key'], file_hash)
        
        # Don't upload to S3, reuse existing
        return {"file_id": new_file_id, "deduplicated": True}
    
    # Upload new file
    s3_key = f"users/{user_id}/{uuid.uuid4()}"
    await s3.upload(s3_key, file_data)
    
    return {"file_id": file_id, "deduplicated": False}
```

### Access Control

**Permission Levels**
- **Owner**: Full access (read, write, delete, share)
- **Write**: Can modify file
- **Read**: Can only download

**Permission Check**
```python
async def check_permission(file_id, user_id, required_permission):
    file_meta = await db.get(file_id)
    
    # Owner has all permissions
    if file_meta['owner_id'] == user_id:
        return True
    
    # Check if file is public
    if file_meta['is_public'] and required_permission == 'read':
        return True
    
    # Check explicit permissions
    user_permission = file_meta['permissions'].get(user_id)
    
    permission_hierarchy = {'read': 1, 'write': 2, 'delete': 3}
    
    return (permission_hierarchy.get(user_permission, 0) >= 
            permission_hierarchy.get(required_permission, 0))
```

### Key Design Decisions

1. **Pre-signed URLs**
   - Offload upload/download to S3
   - Reduce API server load
   - Direct client-to-storage communication

2. **Multipart Upload**
   - For files >5MB
   - Parallel part uploads
   - Resume capability

3. **Versioning**
   - S3 versioning enabled
   - Keep previous versions for 30 days
   - Metadata tracks current version

4. **CDN for Public Files**
   - CloudFront for public files
   - Cache headers for static content
   - Geo-distribution

5. **Storage Lifecycle**
   - Standard: Active files
   - IA (Infrequent Access): After 30 days
   - Glacier: After 90 days (archives)

### API Endpoints

```
POST   /api/v1/files/upload-init
POST   /api/v1/files/{id}/finalize
GET    /api/v1/files/{id}/download
DELETE /api/v1/files/{id}
POST   /api/v1/files/{id}/share
GET    /api/v1/files/list
```

---

## 10. Flight Booking System

### Requirements
- Handle booking contention
- Payment processing
- Failure handling
- Sync with aggregators

### Architecture Diagram

```
┌───────────┐
│   User    │
└─────┬─────┘
      │
┌─────▼──────────────────────┐
│   Booking Service          │
└─────┬──────────────────────┘
      │
┌─────▼──────────────────────┐
│   Inventory Lock Service   │
│   (Redis: SETNX for lock)  │
└─────┬──────────────────────┘
      │
┌─────▼──────────────────────┐
│   Payment Service          │
│   (Saga pattern)           │
└─────┬──────────────────────┘
      │
┌─────▼──────────────────────┐
│   Confirmation Service     │
└─────┬──────────────────────┘
      │
┌─────▼──────────────────────┐
│   Sync to Aggregators      │
│   (Async, eventual)        │
└────────────────────────────┘

Database:
┌────────────────────────────┐
│   Bookings (PostgreSQL)    │
│   - booking_id, status     │
│   - flight_id, user_id     │
└────────────────────────────┘

┌────────────────────────────┐
│   Inventory (PostgreSQL)   │
│   - flight_id, seat_num    │
│   - status, version        │
└────────────────────────────┘
```

### Data Schema

**Flights Table**
```sql
CREATE TABLE flights (
    flight_id BIGSERIAL PRIMARY KEY,
    flight_number VARCHAR(20) NOT NULL,
    airline VARCHAR(100),
    origin VARCHAR(10),
    destination VARCHAR(10),
    departure_time TIMESTAMP,
    arrival_time TIMESTAMP,
    total_seats INT,
    available_seats INT,
    base_price DECIMAL(10, 2),
    version INT DEFAULT 0 -- For optimistic locking
);
```

**Inventory Table**
```sql
CREATE TABLE seat_inventory (
    seat_id BIGSERIAL PRIMARY KEY,
    flight_id BIGINT REFERENCES flights(flight_id),
    seat_number VARCHAR(10),
    seat_class VARCHAR(20), -- 'economy', 'business', 'first'
    status VARCHAR(20), -- 'available', 'locked', 'booked'
    locked_at TIMESTAMP,
    locked_by UUID, -- Booking session ID
    price DECIMAL(10, 2),
    version INT DEFAULT 0,
    UNIQUE(flight_id, seat_number)
);

CREATE INDEX idx_seat_flight ON seat_inventory(flight_id, status);
```

**Bookings Table**
```sql
CREATE TABLE bookings (
    booking_id UUID PRIMARY KEY,
    user_id BIGINT NOT NULL,
    flight_id BIGINT REFERENCES flights(flight_id),
    seat_ids BIGINT[], -- Array of seat IDs
    total_price DECIMAL(10, 2),
    status VARCHAR(20), -- 'pending', 'confirmed', 'cancelled', 'failed'
    payment_id UUID,
    created_at TIMESTAMP DEFAULT NOW(),
    confirmed_at TIMESTAMP,
    expires_at TIMESTAMP -- Auto-cancel if not confirmed in 10 mins
);

CREATE INDEX idx_bookings_user ON bookings(user_id, created_at DESC);
CREATE INDEX idx_bookings_status ON bookings(status);
```

**Saga State Table**
```sql
CREATE TABLE booking_saga (
    saga_id UUID PRIMARY KEY,
    booking_id UUID REFERENCES bookings(booking_id),
    current_step VARCHAR(50),
    status VARCHAR(20), -- 'in_progress', 'completed', 'failed', 'compensating'
    steps_completed JSONB,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);
```

### Booking Contention Handling

**1. Pessimistic Locking (Redis)**
```python
async def lock_seats(flight_id, seat_numbers, session_id):
    lock_key = f"seat_lock:{flight_id}"
    
    # Try to acquire lock with SETNX
    acquired = await redis.setnx(lock_key, session_id)
    
    if not acquired:
        raise BookingContentionError("Seats are being booked by another user")
    
    # Set TTL (10 minutes)
    await redis.expire(lock_key, 600)
    
    try:
        # Check and lock seats
        for seat_num in seat_numbers:
            result = await db.execute("""
                UPDATE seat_inventory
                SET status = 'locked',
                    locked_at = NOW(),
                    locked_by = $1
                WHERE flight_id = $2
                  AND seat_number = $3
                  AND status = 'available'
                RETURNING seat_id
            """, session_id, flight_id, seat_num)
            
            if not result:
                raise SeatUnavailableError(f"Seat {seat_num} not available")
        
        return True
        
    finally:
        # Release lock
        await redis.delete(lock_key)
```

**2. Optimistic Locking (Database Version)**
```python
async def book_seat_optimistic(seat_id, session_id):
    # Read current version
    seat = await db.fetch_one("""
        SELECT seat_id, version, status
        FROM seat_inventory
        WHERE seat_id = $1
    """, seat_id)
    
    if seat['status'] != 'available':
        raise SeatUnavailableError()
    
    # Try to update with version check
    result = await db.execute("""
        UPDATE seat_inventory
        SET status = 'locked',
            locked_by = $1,
            version = version + 1
        WHERE seat_id = $2
          AND version = $3
          AND status = 'available'
        RETURNING seat_id
    """, session_id, seat_id, seat['version'])
    
    if not result:
        # Version mismatch or seat taken
        raise BookingContentionError("Seat was booked by another user")
    
    return True
```

**3. Prevent Double Booking**
```sql
-- Unique constraint
ALTER TABLE seat_inventory
ADD CONSTRAINT unique_flight_seat UNIQUE (flight_id, seat_number);

-- Check constraint
ALTER TABLE seat_inventory
ADD CONSTRAINT valid_status CHECK (status IN ('available', 'locked', 'booked'));
```

### Saga Pattern for Payment

**Booking Saga Steps**
```
1. Reserve Seat(s)
2. Initiate Payment
3. Confirm Payment
4. Confirm Booking
5. Send Confirmation Email
6. Sync to Aggregators
```

**Saga Implementation**
```python
class BookingSaga:
    def __init__(self, booking_id):
        self.booking_id = booking_id
        self.saga_id = str(uuid.uuid4())
        
    async def execute(self):
        try:
            # Step 1: Reserve seats
            await self.reserve_seats()
            await self.update_saga('reserve_seats', 'completed')
            
            # Step 2: Process payment
            payment_result = await self.process_payment()
            await self.update_saga('payment', 'completed')
            
            # Step 3: Confirm booking
            await self.confirm_booking()
            await self.update_saga('confirm', 'completed')
            
            # Step 4: Send confirmation
            await self.send_confirmation()
            
            # Step 5: Async sync to aggregators
            await self.queue_aggregator_sync()
            
            return {"status": "success", "booking_id": self.booking_id}
            
        except PaymentFailedError:
            # Compensate: Release seats
            await self.compensate_payment_failure()
            raise
            
        except Exception as e:
            # General failure compensation
            await self.compensate()
            raise
    
    async def compensate_payment_failure(self):
        # Release locked seats
        await db.execute("""
            UPDATE seat_inventory
            SET status = 'available',
                locked_by = NULL,
                locked_at = NULL
            WHERE locked_by = $1
        """, self.booking_id)
        
        # Mark booking as failed
        await db.execute("""
            UPDATE bookings
            SET status = 'failed'
            WHERE booking_id = $1
        """, self.booking_id)
```

### Payment Failure Handling

**Retry Strategy**
```python
async def process_payment_with_retry(booking):
    max_retries = 3
    backoff_seconds = [1, 2, 4]
    
    for attempt in range(max_retries):
        try:
            result = await payment_gateway.charge(
                amount=booking['total_price'],
                currency='INR',
                booking_id=booking['booking_id']
            )
            
            # Update booking with payment ID
            await db.execute("""
                UPDATE bookings
                SET payment_id = $1,
                    status = 'confirmed'
                WHERE booking_id = $2
            """, result['payment_id'], booking['booking_id'])
            
            return result
            
        except NetworkError as e:
            if attempt < max_retries - 1:
                await asyncio.sleep(backoff_seconds[attempt])
                continue
            else:
                # All retries failed
                raise PaymentFailedError()
                
        except InsufficientFundsError:
            # Don't retry, fail immediately
            raise PaymentFailedError("Insufficient funds")
```

### Sync to Aggregators

**Async Synchronization**
```python
# Queue message for aggregator sync
async def queue_aggregator_sync(booking_id):
    await kafka_producer.send('aggregator-sync', {
        'booking_id': booking_id,
        'action': 'new_booking',
        'timestamp': datetime.now().isoformat()
    })

# Consumer (separate service)
async def sync_to_aggregators(message):
    booking = await get_booking(message['booking_id'])
    
    aggregators = ['expedia', 'makemytrip', 'goibibo']
    
    for aggregator in aggregators:
        try:
            await aggregator_client.sync_booking(
                aggregator,
                booking
            )
        except Exception as e:
            logger.error(f"Sync failed for {aggregator}: {e}")
            
            # Retry later (dead letter queue)
            await kafka_producer.send('aggregator-sync-dlq', {
                'booking_id': booking['booking_id'],
                'aggregator': aggregator,
                'retry_count': message.get('retry_count', 0) + 1
            })
```

**Retry with Exponential Backoff**
```python
async def retry_failed_syncs():
    while True:
        # Process dead letter queue
        messages = await kafka_consumer.poll('aggregator-sync-dlq')
        
        for msg in messages:
            if msg['retry_count'] > 5:
                # Give up after 5 retries
                logger.error(f"Max retries exceeded: {msg}")
                continue
            
            # Exponential backoff
            delay = 2 ** msg['retry_count']
            await asyncio.sleep(delay)
            
            # Retry sync
            try:
                await aggregator_client.sync_booking(
                    msg['aggregator'],
                    msg['booking_id']
                )
            except Exception:
                # Re-queue with incremented retry count
                await kafka_producer.send('aggregator-sync-dlq', {
                    ...msg,
                    'retry_count': msg['retry_count'] + 1
                })
```

### Key Design Decisions

1. **Booking Contention**
   - **Redis lock**: Prevents concurrent bookings of same seats
   - **Optimistic locking**: Version field in database
   - **Unique constraint**: Final safeguard against double booking

2. **Saga Pattern**
   - **Orchestration**: Centralized saga coordinator
   - **Compensation**: Automatic rollback on failure
   - **State persistence**: Track saga progress in DB

3. **Payment Handling**
   - **Idempotency**: Unique booking_id prevents duplicate charges
   - **Retry**: Exponential backoff for transient failures
   - **Timeout**: Auto-cancel booking after 10 minutes

4. **Aggregator Sync**
   - **Async**: Don't block booking confirmation
   - **Eventual consistency**: Acceptable delay for sync
   - **Retry queue**: Handle aggregator downtime

5. **Auto-expiry**
   - **Background job**: Runs every minute
   ```sql
   UPDATE bookings
   SET status = 'cancelled'
   WHERE status = 'pending'
     AND expires_at < NOW()
   ```
   - Release locked seats automatically

### API Endpoints

```
POST   /api/v1/bookings/search-flights
POST   /api/v1/bookings/initiate
POST   /api/v1/bookings/{id}/payment
GET    /api/v1/bookings/{id}
DELETE /api/v1/bookings/{id}/cancel
```

---

## Summary

This document covers 10 comprehensive system designs with:
- Architecture diagrams
- Data schemas
- Key design decisions
- Scalability considerations
- Fault tolerance mechanisms
- API specifications

Each design balances simplicity with completeness, suitable for technical interviews and real-world implementation.